{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb854a0",
   "metadata": {},
   "source": [
    "# Structural + CSR analysis\n",
    "\n",
    "Objectives of this notebook:\n",
    "\n",
    "- **Per layer:**\n",
    "  - `nonzero`, `total`, `sparsity`\n",
    "  - dimensions (shape of the main weight tensor)\n",
    "  - estimated FLOPs per token for linear / CSR layers\n",
    "- **Per group:**\n",
    "  - `embedding`\n",
    "  - `attention_linear` (Q/K/V/out)\n",
    "  - `mlp_linear` (fully-connected layers in the MLP)\n",
    "  - `lm_head`\n",
    "  - `other_*`\n",
    "\n",
    "We compare three setups:\n",
    "\n",
    "- **Dense**\n",
    "- **Masked30** (global 30% pruning with dense execution)\n",
    "- **CSR30** (same pruning, CSR conversion for all prunable linears)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fb8ed9",
   "metadata": {},
   "source": [
    "# What the metrics mean in this notebook (clear explanation)\n",
    "\n",
    "This notebook performs a **structural analysis** of a Transformer model for three variants:\n",
    "\n",
    "* **Dense** — original model\n",
    "* **Masked30** — 30% global magnitude pruning (weights set to 0, tensors stay dense)\n",
    "* **CSR30** — same pruning, but selected Linear layers are replaced by `LinearCSRForward` (true sparse CSR matrices)\n",
    "\n",
    "The goal is to understand:\n",
    "-> **Where parameters live**\n",
    "-> **Where sparsity appears**\n",
    "-> **How much theoretical compute is removed**\n",
    "-> **How the structure differs across Dense / Masked / CSR**\n",
    "\n",
    "---\n",
    "\n",
    "## Per-layer metrics (`df_layers_*`)\n",
    "\n",
    "Each **row** in `df_layers` corresponds to **one module inside the model**.\n",
    "Example rows:\n",
    "\n",
    "* `model.decoder.layers.0.self_attn.q_proj`\n",
    "* `model.decoder.layers.3.mlp.fc1`\n",
    "* `model.decoder.embed_tokens`\n",
    "* `lm_head`\n",
    "* etc.\n",
    "\n",
    "For each module we record:\n",
    "\n",
    "### **Basic identifiers**\n",
    "\n",
    "* **`model`** — which variant this row belongs to (`Dense`, `Masked30`, `CSR30`)\n",
    "* **`module_name`** — full dotted name inside the model\n",
    "* **`group`** — high-level category:\n",
    "\n",
    "  * `embedding`\n",
    "  * `attention_linear` (q/k/v/out projections)\n",
    "  * `mlp_linear` (feed-forward layers)\n",
    "  * `lm_head`\n",
    "  * `norm`\n",
    "  * `other_linear` / `other`\n",
    "\n",
    "### **Parameter statistics**\n",
    "\n",
    "* **`nonzero`** — number of parameters ≠ 0\n",
    "* **`total`** — total parameters\n",
    "* **`sparsity`** — fraction of zeros:\n",
    "\n",
    "$\\text{sparsity} = 1 - \\frac{\\text{nonzero}}{\\text{total}}$\n",
    "\n",
    "* **`shape`** — weight matrix shape (e.g. `(out, in)` for Linear)\n",
    "\n",
    "### **Compute estimate**\n",
    "\n",
    "* **Dense Linear**:\n",
    "  FLOPs per token ≈ `2 * in_features * out_features`\n",
    "* **CSR Linear**:\n",
    "  FLOPs per token ≈ `2 * nnz`\n",
    "  (only non-zeros count)\n",
    "\n",
    "### **Parameter share**\n",
    "\n",
    "* **`param_frac`** — proportion of model parameters in that module:\n",
    "\n",
    "$\\text{param_frac} = \\frac{\\text{total parameters in layer}}{\\text{total parameters in model}}$\n",
    "\n",
    "-> This lets you zoom in on **specific layers** and see how pruning alters their structure.\n",
    "\n",
    "---\n",
    "\n",
    "## Group-level metrics (`df_groups_*`)\n",
    "\n",
    "We aggregate the layer-level stats by `(model, group)`.\n",
    "\n",
    "For each group (e.g., all attention projections):\n",
    "\n",
    "* **`nonzero`** — sum of non-zeros across modules\n",
    "* **`total`** — total parameters\n",
    "* **`sparsity`**:\n",
    "\n",
    "$\\text{sparsity}*{\\text{group}} = 1 - \\frac{\\text{nonzero}*{\\text{group}}}{\\text{total}_{\\text{group}}}$\n",
    "\n",
    "* **`flops_per_token`** — sum of FLOPs for all modules in this group\n",
    "* **`param_frac`** — fraction of the model’s parameters inside this group\n",
    "\n",
    "-> This highlights which *parts of the network* dominate size and compute.\n",
    "\n",
    "---\n",
    "\n",
    "## How to interpret the plots\n",
    "\n",
    "The notebook generates three comparisons across **Dense / Masked30 / CSR30**:\n",
    "\n",
    "### **1. Group sparsity**\n",
    "\n",
    "Shows **where zeroing actually happens**.\n",
    "\n",
    "* Embeddings / lm_head remain dense (excluded from pruning)\n",
    "* Attention and MLP linear layers become sparse\n",
    "\n",
    "### **2. Parameter share**\n",
    "\n",
    "Tells you **where parameters naturally live**, regardless of sparsity.\n",
    "\n",
    "* MLP layers → large fraction\n",
    "* Attention projections → significant\n",
    "* Embeddings → often huge chunk in small LLMs\n",
    "\n",
    "### **3. FLOPs per token**\n",
    "\n",
    "A **theoretical compute estimate**:\n",
    "\n",
    "* Dense compute ∝ number of weight multiplications\n",
    "* CSR compute ∝ number of non-zeros\n",
    "* Masked compute ≈ Dense compute (dense kernels ignore zeros)\n",
    "\n",
    "This illustrates:\n",
    "\n",
    "* Masked pruning is **structural sparsity only** (no speedup)\n",
    "* CSR pruning is **algorithmic sparsity** (real reduction in multiply-adds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246927f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, sys, warnings, pandas as pd, torch, torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "sys.path.append('..'); sys.path.append('../src')\n",
    "\n",
    "from src.eval.metrics import params_size_and_sparsity\n",
    "from src.eval.plotting import bar_plot\n",
    "from src.pruning.policies import apply_global_magnitude_pruning_cpu_safe, select_prunable_linears\n",
    "from src.pruning.pipeline import freeze_pruning_, convert_linear_weights_to_csr_\n",
    "from src.wrappers.linear_csr import LinearCSRForward\n",
    "\n",
    "warnings.filterwarnings('ignore', message='.*Sparse CSR tensor support is in beta state.*')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "RESULTS_DIR = os.path.join('..', 'results')\n",
    "STRUCT_DIR = os.path.join(RESULTS_DIR, 'structural_layers')\n",
    "os.makedirs(STRUCT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e54320f",
   "metadata": {},
   "source": [
    "## 1. Model loading\n",
    "\n",
    "We load a small model depending on the device:\n",
    "\n",
    "- On **GPU**: `EleutherAI/pythia-410m` in fp16\n",
    "- On **CPU**: `facebook/opt-125m` in fp32\n",
    "\n",
    "If you want to use a local snapshot (for example on Narval), simply replace `model_name` with the local path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13cca1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fresh():\n",
    "    \"\"\"\n",
    "    Load a small model depending on the device.\n",
    "\n",
    "    - CUDA -> EleutherAI/pythia-410m (fp16)\n",
    "    - CPU  -> facebook/opt-125m     (fp32)\n",
    "    \"\"\"\n",
    "    if device == \"cuda\":\n",
    "        model_name = \"EleutherAI/pythia-410m\"\n",
    "        torch_dtype = torch.float16\n",
    "    else:\n",
    "        model_name = \"facebook/opt-125m\"\n",
    "        torch_dtype = None  # fp32\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    kwargs = {}\n",
    "    if torch_dtype is not None:\n",
    "        kwargs[\"torch_dtype\"] = torch_dtype\n",
    "\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        **kwargs\n",
    "    ).to(device).eval()\n",
    "    print(f\"Loaded: {model_name}\")\n",
    "    return mdl, tok, model_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a55f91b",
   "metadata": {},
   "source": [
    "## 2. Per-layer analysis helpers\n",
    "\n",
    "We define the following utilities:\n",
    "\n",
    "- `tensor_stats(t)` → `(nonzero, total)`\n",
    "- `linear_flops(weight)` → approximate FLOPs per token for a linear layer (≈ `2 * in * out`)\n",
    "- `classify_module(name, module)` → assign each module to a group (`embedding`, `attention_linear`, `mlp_linear`, `lm_head`, etc.)\n",
    "- `analyze_layers(model, label)` → returns:\n",
    "  - a *per-layer* DataFrame, and\n",
    "  - a *per-group* aggregation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f40a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_stats(t: torch.Tensor):\n",
    "    if t.numel() == 0:\n",
    "        return 0, 0\n",
    "    nnz = int((t != 0).sum().item())\n",
    "    total = t.numel()\n",
    "    return nnz, total\n",
    "\n",
    "def linear_flops(weight: torch.Tensor):\n",
    "    \"\"\"Approximate FLOPs per token for a linear layer.\n",
    "    We count 2 * in * out (mul + add).\"\"\"\n",
    "    if weight is None or weight.dim() != 2:\n",
    "        return 0\n",
    "    out_features, in_features = weight.shape\n",
    "    return int(2 * in_features * out_features)\n",
    "\n",
    "def classify_module(name: str, module: nn.Module) -> str:\n",
    "    lname = name.lower()\n",
    "\n",
    "    if isinstance(module, nn.Embedding) or 'embed' in lname:\n",
    "        return 'embedding'\n",
    "\n",
    "    if 'lm_head' in lname:\n",
    "        return 'lm_head'\n",
    "\n",
    "    if isinstance(module, (nn.Linear, LinearCSRForward)):\n",
    "        # Attention\n",
    "        if 'attn' in lname or 'attention' in lname or 'self_attn' in lname:\n",
    "            return 'attention_linear'\n",
    "        # MLP\n",
    "        if 'mlp' in lname or 'ff' in lname or 'fc1' in lname or 'fc2' in lname:\n",
    "            return 'mlp_linear'\n",
    "        return 'other_linear'\n",
    "\n",
    "    if 'norm' in lname:\n",
    "        return 'norm'\n",
    "\n",
    "    return 'other'\n",
    "\n",
    "def analyze_layers(model: nn.Module, label: str):\n",
    "    rows = []\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if name == '':\n",
    "            continue\n",
    "\n",
    "        nonzero = 0\n",
    "        total = 0\n",
    "        flops = 0\n",
    "        shape = None\n",
    "\n",
    "        if isinstance(module, LinearCSRForward):\n",
    "            # Read CSR meta-data\n",
    "            nonzero = module.meta_nnz\n",
    "            total = module.meta_total_params\n",
    "            sparsity = module.meta_sparsity\n",
    "            shape = tuple(module.meta_dense_shape.tolist())\n",
    "            # FLOPs ≈ 2 * nnz (mul + add)\n",
    "            flops = 2 * nonzero\n",
    "        else:\n",
    "            # Standard path (nn.Linear, Embedding, etc.)\n",
    "            for p_name, p in module.named_parameters(recurse=False):\n",
    "                if not isinstance(p, torch.Tensor):\n",
    "                    continue\n",
    "                nnz, tot = tensor_stats(p)\n",
    "                nonzero += nnz\n",
    "                total += tot\n",
    "                if p_name in ('weight', 'weight_orig'):\n",
    "                    flops += linear_flops(p)\n",
    "                    if p.dim() == 2:\n",
    "                        shape = tuple(p.shape)\n",
    "\n",
    "            if total == 0:\n",
    "                continue\n",
    "            sparsity = 1.0 - nonzero / total\n",
    "\n",
    "        group = classify_module(name, module)\n",
    "        rows.append({\n",
    "            'model': label,\n",
    "            'module_name': name,\n",
    "            'group': group,\n",
    "            'nonzero': nonzero,\n",
    "            'total': total,\n",
    "            'sparsity': sparsity,\n",
    "            'shape': str(shape) if shape is not None else '',\n",
    "            'flops_per_token': flops\n",
    "        })\n",
    "\n",
    "    df_layers = pd.DataFrame(rows)\n",
    "    total_params_model = df_layers['total'].sum()\n",
    "    df_layers['param_frac'] = df_layers['total'] / total_params_model\n",
    "\n",
    "    df_groups = (\n",
    "        df_layers\n",
    "        .groupby(['model', 'group'], as_index=False)\n",
    "        .agg({'nonzero': 'sum', 'total': 'sum', 'flops_per_token': 'sum'})\n",
    "    )\n",
    "    df_groups['sparsity'] = 1.0 - df_groups['nonzero'] / df_groups['total']\n",
    "    df_groups['param_frac'] = df_groups['total'] / total_params_model\n",
    "\n",
    "    return df_layers, df_groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35327385",
   "metadata": {},
   "source": [
    "## 3. Dense — baseline structure\n",
    "\n",
    "We first analyse the **unpruned dense model**.\n",
    "\n",
    "For this variant we compute for each layer and group:\n",
    "\n",
    "- number of non-zero parameters,\n",
    "- total parameter count,\n",
    "- sparsity,\n",
    "- parameter fraction within the model,\n",
    "- estimated FLOPs per token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d82a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: facebook/opt-125m\n",
      "Dense global stats: {'nonzero': 125238422, 'total': 125239296, 'sparsity': 6.978640314292406e-06, 'size_mb': 477.75}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>group</th>\n",
       "      <th>nonzero</th>\n",
       "      <th>total</th>\n",
       "      <th>flops_per_token</th>\n",
       "      <th>sparsity</th>\n",
       "      <th>param_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dense</td>\n",
       "      <td>attention_linear</td>\n",
       "      <td>21673527</td>\n",
       "      <td>21673536</td>\n",
       "      <td>56623560423104</td>\n",
       "      <td>4.152638e-07</td>\n",
       "      <td>0.173004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dense</td>\n",
       "      <td>embedding</td>\n",
       "      <td>30671751</td>\n",
       "      <td>30671808</td>\n",
       "      <td>80366959266592</td>\n",
       "      <td>1.862567e-06</td>\n",
       "      <td>0.244873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dense</td>\n",
       "      <td>lm_head</td>\n",
       "      <td>29598169</td>\n",
       "      <td>29598208</td>\n",
       "      <td>77217792721792</td>\n",
       "      <td>1.315463e-06</td>\n",
       "      <td>0.236490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dense</td>\n",
       "      <td>mlp_linear</td>\n",
       "      <td>43384793</td>\n",
       "      <td>43384832</td>\n",
       "      <td>113246208462208</td>\n",
       "      <td>9.028209e-07</td>\n",
       "      <td>0.346477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dense</td>\n",
       "      <td>norm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model            group   nonzero     total  flops_per_token      sparsity  \\\n",
       "0  Dense  attention_linear  21673527  21673536    56623560423104  4.152638e-07   \n",
       "1  Dense         embedding  30671751  30671808    80366959266592  1.862567e-06   \n",
       "2  Dense           lm_head  29598169  29598208    77217792721792  1.315463e-06   \n",
       "3  Dense        mlp_linear  43384793  43384832   113246208462208  9.028209e-07   \n",
       "4  Dense              norm         0         0                0           NaN   \n",
       "\n",
       "   param_frac  \n",
       "0    0.173004  \n",
       "1    0.244873  \n",
       "2    0.236490  \n",
       "3    0.346477  \n",
       "4         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ..\\results\\structural_layers\\layers_dense.csv\n",
      "Saved: ..\\results\\structural_layers\\groups_dense.csv\n",
      "Saved: ..\\results\\structural_layers\\dense_sparsity_per_group.png\n",
      "Saved: ..\\results\\structural_layers\\dense_param_frac_per_group.png\n",
      "Saved: ..\\results\\structural_layers\\dense_flops_per_group.png\n"
     ]
    }
   ],
   "source": [
    "model_dense, tok, model_name = load_fresh()\n",
    "dense_stats = params_size_and_sparsity(model_dense)\n",
    "print('Dense global stats:', dense_stats)\n",
    "\n",
    "df_layers_dense, df_groups_dense = analyze_layers(model_dense, 'Dense')\n",
    "display(df_groups_dense.sort_values('total', ascending=False))\n",
    "\n",
    "dense_layers_csv = os.path.join(STRUCT_DIR, 'layers_dense.csv')\n",
    "dense_groups_csv = os.path.join(STRUCT_DIR, 'groups_dense.csv')\n",
    "df_layers_dense.to_csv(dense_layers_csv, index=False)\n",
    "df_groups_dense.to_csv(dense_groups_csv, index=False)\n",
    "print('Saved:', dense_layers_csv)\n",
    "print('Saved:', dense_groups_csv)\n",
    "\n",
    "# Overview plots\n",
    "bar_plot(df_groups_dense, 'group', 'sparsity', 'Dense: sparsity per group', 'dense_sparsity_per_group.png', STRUCT_DIR, y_min=0.0)\n",
    "bar_plot(df_groups_dense, 'group', 'param_frac', 'Dense: parameter share per group', 'dense_param_frac_per_group.png', STRUCT_DIR, y_min=0.0)\n",
    "bar_plot(df_groups_dense, 'group', 'flops_per_token', 'Dense: FLOPs per token per group', 'dense_flops_per_group.png', STRUCT_DIR, y_min=0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd3ac32",
   "metadata": {},
   "source": [
    "## 4. Masked30 — global magnitude pruning (30%) on prunable linears\n",
    "\n",
    "We now apply global magnitude pruning (30%) on the prunable linear layers:\n",
    "\n",
    "- We prune only the `nn.Linear` modules returned by `select_prunable_linears`.\n",
    "- The `lm_head` is blacklisted.\n",
    "- Embeddings and LayerNorms are never pruned.\n",
    "\n",
    "We then inspect the effect on:\n",
    "\n",
    "- sparsity per layer / per group,\n",
    "- theoretical FLOPs per token per group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "100574b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP = 0.3...\n"
     ]
    }
   ],
   "source": [
    "SP = 0.30\n",
    "\n",
    "model_masked, tok_m, _ = load_fresh()\n",
    "layers_prunable = select_prunable_linears(model_masked, blacklist=(\"lm_head\",))\n",
    "print('Prunable linear layers:', len(layers_prunable))\n",
    "\n",
    "apply_global_magnitude_pruning_cpu_safe(layers_prunable, amount=SP)\n",
    "freeze_pruning_(layers_prunable)\n",
    "\n",
    "masked_stats = params_size_and_sparsity(model_masked)\n",
    "print('Masked global stats:', masked_stats)\n",
    "\n",
    "df_layers_masked, df_groups_masked = analyze_layers(model_masked, f'Masked{int(SP*100)}')\n",
    "display(df_groups_masked.sort_values('total', ascending=False))\n",
    "\n",
    "masked_layers_csv = os.path.join(STRUCT_DIR, 'layers_masked30.csv')\n",
    "masked_groups_csv = os.path.join(STRUCT_DIR, 'groups_masked30.csv')\n",
    "df_layers_masked.to_csv(masked_layers_csv, index=False)\n",
    "df_groups_masked.to_csv(masked_groups_csv, index=False)\n",
    "print('Saved:', masked_layers_csv)\n",
    "print('Saved:', masked_groups_csv)\n",
    "\n",
    "bar_plot(df_groups_masked, 'group', 'sparsity', 'Masked30: sparsity per group', 'masked30_sparsity_per_group.png', STRUCT_DIR, y_min=0.0)\n",
    "bar_plot(df_groups_masked, 'group', 'param_frac', 'Masked30: parameter share per group', 'masked30_param_frac_per_group.png', STRUCT_DIR, y_min=0.0)\n",
    "bar_plot(df_groups_masked, 'group', 'flops_per_token', 'Masked30: FLOPs per token per group', 'masked30_flops_per_group.png', STRUCT_DIR, y_min=0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f8741d",
   "metadata": {},
   "source": [
    "## 5. CSR30 — pruning + CSR conversion (GPU-ready)\n",
    "\n",
    "The pipeline for the CSR variant is:\n",
    "\n",
    "1. Load a fresh model.\n",
    "2. Apply the same global 30% pruning on prunable linear layers.\n",
    "3. Call `freeze_pruning_` to materialise the masks.\n",
    "4. Run `convert_linear_weights_to_csr_`.\n",
    "5. Replace all pruned linear layers by `LinearCSRForward` modules.\n",
    "\n",
    "We then repeat the layer- and group-level analysis.\n",
    "\n",
    "> On CPU this is slower, but on GPU this corresponds to the “CSR, GPU-ready” configuration that we will benchmark on Narval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28f42487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: facebook/opt-125m\n",
      "Prunable linear layers (CSR): ...\n"
     ]
    }
   ],
   "source": [
    "model_csr, tok_c, _ = load_fresh()\n",
    "layers_prunable_csr = select_prunable_linears(model_csr, blacklist=(\"lm_head\",))\n",
    "print('Prunable linear layers (CSR):', len(layers_prunable_csr))\n",
    "\n",
    "apply_global_magnitude_pruning_cpu_safe(layers_prunable_csr, amount=SP)\n",
    "freeze_pruning_(layers_prunable_csr)\n",
    "convert_linear_weights_to_csr_(layers_prunable_csr)\n",
    "\n",
    "# Replace all pruned layers with LinearCSRForward\n",
    "def find_parent(root, child):\n",
    "    for _, mod in root.named_modules():\n",
    "        for cn, cc in mod.named_children():\n",
    "            if cc is child:\n",
    "                return mod, cn\n",
    "    raise RuntimeError('Parent not found')\n",
    "\n",
    "for lin in layers_prunable_csr:\n",
    "    parent, attr = find_parent(model_csr, lin)\n",
    "    csr_module = LinearCSRForward(\n",
    "        lin.weight.detach(),\n",
    "        lin.bias.detach() if lin.bias is not None else None\n",
    "    ).to(device)\n",
    "    setattr(parent, attr, csr_module)\n",
    "\n",
    "csr_stats = params_size_and_sparsity(model_csr)\n",
    "print('CSR global stats:', csr_stats)\n",
    "\n",
    "df_layers_csr, df_groups_csr = analyze_layers(model_csr, f'CSR{int(SP*100)}')\n",
    "display(df_groups_csr.sort_values('total', ascending=False))\n",
    "\n",
    "csr_layers_csv = os.path.join(STRUCT_DIR, 'layers_csr30.csv')\n",
    "csr_groups_csv = os.path.join(STRUCT_DIR, 'groups_csr30.csv')\n",
    "df_layers_csr.to_csv(csr_layers_csv, index=False)\n",
    "df_groups_csr.to_csv(csr_groups_csv, index=False)\n",
    "print('Saved:', csr_layers_csv)\n",
    "print('Saved:', csr_groups_csv)\n",
    "\n",
    "bar_plot(df_groups_csr, 'group', 'sparsity', 'CSR30: sparsity per group', 'csr30_sparsity_per_group.png', STRUCT_DIR, y_min=0.0)\n",
    "bar_plot(df_groups_csr, 'group', 'param_frac', 'CSR30: parameter share per group', 'csr30_param_frac_per_group.png', STRUCT_DIR, y_min=0.0)\n",
    "bar_plot(df_groups_csr, 'group', 'flops_per_token', 'CSR30: FLOPs per token per group', 'csr30_flops_per_group.png', STRUCT_DIR, y_min=0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160b56d7",
   "metadata": {},
   "source": [
    "## 6. Global comparison: Dense vs Masked30 vs CSR30\n",
    "\n",
    "We merge the per-group DataFrames to build an overview with:\n",
    "\n",
    "- sparsity per group and per variant,\n",
    "- parameter share per group and per variant,\n",
    "- total FLOPs per token per group and per variant.\n",
    "\n",
    "This makes it easy to see where parameters and compute are concentrated, and how pruning + CSR change the picture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da26530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg_d = df_groups_dense.copy();   dfg_d['variant'] = 'Dense'\n",
    "dfg_m = df_groups_masked.copy();  dfg_m['variant'] = 'Masked30'\n",
    "dfg_c = df_groups_csr.copy();     dfg_c['variant'] = 'CSR30'\n",
    "\n",
    "dfg_all = pd.concat([dfg_d, dfg_m, dfg_c], ignore_index=True)\n",
    "display(dfg_all.sort_values(['group', 'variant']))\n",
    "\n",
    "pivot_sparsity = dfg_all.pivot(index='group', columns='variant', values='sparsity').fillna(0.0)\n",
    "pivot_frac = dfg_all.pivot(index='group', columns='variant', values='param_frac').fillna(0.0)\n",
    "pivot_flops = dfg_all.pivot(index='group', columns='variant', values='flops_per_token').fillna(0.0)\n",
    "\n",
    "print('\\nSparsity per group:')\n",
    "display(pivot_sparsity)\n",
    "print('\\nParameter share per group:')\n",
    "display(pivot_frac)\n",
    "print('\\nFLOPs per token per group:')\n",
    "display(pivot_flops)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf936a28",
   "metadata": {},
   "source": [
    "## 7. Using these results in the report\n",
    "\n",
    "With this notebook you can document, with quantitative evidence:\n",
    "\n",
    "- **Parameter distribution** across groups: embeddings vs attention vs MLP vs `lm_head` (`param_frac`).\n",
    "- **Where sparsity actually appears**: `sparsity` per group, highlighting that embeddings and the output head typically remain dense.\n",
    "- **Impact on theoretical compute**: `flops_per_token` per group and per variant, which lets you argue about potential FLOPs savings if CSR kernels are efficient.\n",
    "\n",
    "You can also zoom in *per layer* using the `df_layers_*` tables to show, for example, that:\n",
    "\n",
    "- MLP layers often contain more parameters and benefit more from pruning than attention projections.\n",
    "- Some early or late layers are more sensitive to pruning (which you can correlate with top-1 accuracy or perplexity experiments).\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
