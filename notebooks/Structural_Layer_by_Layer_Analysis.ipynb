{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb854a0",
   "metadata": {},
   "source": [
    "# Structural + CSR analysis\n",
    "\n",
    "Objectives of this notebook:\n",
    "\n",
    "- **Per layer:**\n",
    "  - `nonzero`, `total`, `sparsity`\n",
    "  - dimensions (shape of the main weight tensor)\n",
    "  - estimated FLOPs per token for linear / CSR layers\n",
    "- **Per group:**\n",
    "  - `embedding`\n",
    "  - `attention_linear` (Q/K/V/out)\n",
    "  - `mlp_linear` (fully-connected layers in the MLP)\n",
    "  - `lm_head`\n",
    "  - `other_*`\n",
    "\n",
    "We compare three setups:\n",
    "\n",
    "- **Dense**\n",
    "- **Masked30** (global 30% pruning with dense execution)\n",
    "- **CSR30** (same pruning, CSR conversion for all prunable linears)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fb8ed9",
   "metadata": {},
   "source": [
    "# What the metrics mean in this notebook (clear explanation)\n",
    "\n",
    "This notebook performs a **structural analysis** of a Transformer model for three variants:\n",
    "\n",
    "* **Dense** — original model\n",
    "* **Masked30** — 30% global magnitude pruning (weights set to 0, tensors stay dense)\n",
    "* **CSR30** — same pruning, but selected Linear layers are replaced by `LinearCSRForward` (true sparse CSR matrices)\n",
    "\n",
    "The goal is to understand:\n",
    "-> **Where parameters live**\n",
    "-> **Where sparsity appears**\n",
    "-> **How much theoretical compute is removed**\n",
    "-> **How the structure differs across Dense / Masked / CSR**\n",
    "\n",
    "---\n",
    "\n",
    "## Per-layer metrics (`df_layers_*`)\n",
    "\n",
    "Each **row** in `df_layers` corresponds to **one module inside the model**.\n",
    "Example rows:\n",
    "\n",
    "* `model.decoder.layers.0.self_attn.q_proj`\n",
    "* `model.decoder.layers.3.mlp.fc1`\n",
    "* `model.decoder.embed_tokens`\n",
    "* `lm_head`\n",
    "* etc.\n",
    "\n",
    "For each module we record:\n",
    "\n",
    "### **Basic identifiers**\n",
    "\n",
    "* **`model`** — which variant this row belongs to (`Dense`, `Masked30`, `CSR30`)\n",
    "* **`module_name`** — full dotted name inside the model\n",
    "* **`group`** — high-level category:\n",
    "\n",
    "  * `embedding`\n",
    "  * `attention_linear` (q/k/v/out projections)\n",
    "  * `mlp_linear` (feed-forward layers)\n",
    "  * `lm_head`\n",
    "  * `norm`\n",
    "  * `other_linear` / `other`\n",
    "\n",
    "### **Parameter statistics**\n",
    "\n",
    "* **`nonzero`** — number of parameters ≠ 0\n",
    "* **`total`** — total parameters\n",
    "* **`sparsity`** — fraction of zeros:\n",
    "\n",
    "$\\text{sparsity} = 1 - \\frac{\\text{nonzero}}{\\text{total}}$\n",
    "\n",
    "* **`shape`** — weight matrix shape (e.g. `(out, in)` for Linear)\n",
    "\n",
    "### **Compute estimate**\n",
    "\n",
    "* **Dense Linear**:\n",
    "  FLOPs per token ≈ `2 * in_features * out_features`\n",
    "* **CSR Linear**:\n",
    "  FLOPs per token ≈ `2 * nnz`\n",
    "  (only non-zeros count)\n",
    "\n",
    "### **Parameter share**\n",
    "\n",
    "* **`param_frac`** — proportion of model parameters in that module:\n",
    "\n",
    "$\\text{param_frac} = \\frac{\\text{total parameters in layer}}{\\text{total parameters in model}}$\n",
    "\n",
    "-> This lets you zoom in on **specific layers** and see how pruning alters their structure.\n",
    "\n",
    "---\n",
    "\n",
    "## Group-level metrics (`df_groups_*`)\n",
    "\n",
    "We aggregate the layer-level stats by `(model, group)`.\n",
    "\n",
    "For each group (e.g., all attention projections):\n",
    "\n",
    "* **`nonzero`** — sum of non-zeros across modules\n",
    "* **`total`** — total parameters\n",
    "* **`sparsity`**:\n",
    "\n",
    "$\\text{sparsity}*{\\text{group}} = 1 - \\frac{\\text{nonzero}*{\\text{group}}}{\\text{total}_{\\text{group}}}$\n",
    "\n",
    "* **`flops_per_token`** — sum of FLOPs for all modules in this group\n",
    "* **`param_frac`** — fraction of the model’s parameters inside this group\n",
    "\n",
    "-> This highlights which *parts of the network* dominate size and compute.\n",
    "\n",
    "---\n",
    "\n",
    "## How to interpret the plots\n",
    "\n",
    "The notebook generates three comparisons across **Dense / Masked30 / CSR30**:\n",
    "\n",
    "### **1. Group sparsity**\n",
    "\n",
    "Shows **where zeroing actually happens**.\n",
    "\n",
    "* Embeddings / lm_head remain dense (excluded from pruning)\n",
    "* Attention and MLP linear layers become sparse\n",
    "\n",
    "### **2. Parameter share**\n",
    "\n",
    "Tells you **where parameters naturally live**, regardless of sparsity.\n",
    "\n",
    "* MLP layers → large fraction\n",
    "* Attention projections → significant\n",
    "* Embeddings → often huge chunk in small LLMs\n",
    "\n",
    "### **3. FLOPs per token**\n",
    "\n",
    "A **theoretical compute estimate**:\n",
    "\n",
    "* Dense compute ∝ number of weight multiplications\n",
    "* CSR compute ∝ number of non-zeros\n",
    "* Masked compute ≈ Dense compute (dense kernels ignore zeros)\n",
    "\n",
    "This illustrates:\n",
    "\n",
    "* Masked pruning is **structural sparsity only** (no speedup)\n",
    "* CSR pruning is **algorithmic sparsity** (real reduction in multiply-adds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246927f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, sys, warnings, pandas as pd, torch, torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('..'); sys.path.append('../src')\n",
    "\n",
    "from src.eval.metrics import params_size_and_sparsity\n",
    "from src.pruning.policies import apply_global_magnitude_pruning_cpu_safe, select_prunable_linears\n",
    "from src.pruning.pipeline import freeze_pruning_, convert_linear_weights_to_csr_\n",
    "from src.wrappers.linear_csr import LinearCSRForward\n",
    "\n",
    "warnings.filterwarnings('ignore', message='.*Sparse CSR tensor support is in beta state.*')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "RESULTS_DIR = os.path.join('..', 'results')\n",
    "STRUCT_DIR = os.path.join(RESULTS_DIR, 'structural_layers_bis')\n",
    "os.makedirs(STRUCT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e54320f",
   "metadata": {},
   "source": [
    "## 1. Model loading\n",
    "\n",
    "We load a small model depending on the device:\n",
    "\n",
    "- On **GPU**: `EleutherAI/pythia-410m` in fp16\n",
    "- On **CPU**: `facebook/opt-125m` in fp32\n",
    "\n",
    "If you want to use a local snapshot (for example on Narval), simply replace `model_name` with the local path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13cca1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fresh():\n",
    "    \"\"\"\n",
    "    Load a small model depending on the device.\n",
    "\n",
    "    - CUDA -> EleutherAI/pythia-410m (fp16)\n",
    "    - CPU  -> facebook/opt-125m     (fp32)\n",
    "    \"\"\"\n",
    "    if device == \"cuda\":\n",
    "        model_name = \"EleutherAI/pythia-410m\"\n",
    "        torch_dtype = torch.float16\n",
    "    else:\n",
    "        model_name = \"facebook/opt-125m\"\n",
    "        torch_dtype = None  # fp32\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    kwargs = {}\n",
    "    if torch_dtype is not None:\n",
    "        kwargs[\"torch_dtype\"] = torch_dtype\n",
    "\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        **kwargs\n",
    "    ).to(device).eval()\n",
    "    print(f\"Loaded: {model_name}\")\n",
    "    return mdl, tok, model_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a55f91b",
   "metadata": {},
   "source": [
    "## 2. Per-layer analysis helpers\n",
    "\n",
    "We define the following utilities:\n",
    "\n",
    "- `tensor_stats(t)` → `(nonzero, total)`\n",
    "- `linear_flops(weight)` → approximate FLOPs per token for a linear layer (≈ `2 * in * out`)\n",
    "- `classify_module(name, module)` → assign each module to a group (`embedding`, `attention_linear`, `mlp_linear`, `lm_head`, etc.)\n",
    "- `analyze_layers(model, label)` → returns:\n",
    "  - a *per-layer* DataFrame, and\n",
    "  - a *per-group* aggregation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f40a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_stats(t: torch.Tensor):\n",
    "    if t.numel() == 0:\n",
    "        return 0, 0\n",
    "    nnz = int((t != 0).sum().item())\n",
    "    total = t.numel()\n",
    "    return nnz, total\n",
    "\n",
    "def linear_flops(weight: torch.Tensor):\n",
    "    \"\"\"Approximate FLOPs per token for a linear layer.\n",
    "    We count 2 * in * out (mul + add).\"\"\"\n",
    "    if weight is None or weight.dim() != 2:\n",
    "        return 0\n",
    "    out_features, in_features = weight.shape\n",
    "    return int(2 * in_features * out_features)\n",
    "\n",
    "def classify_module(name: str, module: nn.Module) -> str:\n",
    "    lname = name.lower()\n",
    "\n",
    "    if isinstance(module, nn.Embedding) or 'embed' in lname:\n",
    "        return 'embedding'\n",
    "\n",
    "    if 'lm_head' in lname:\n",
    "        return 'lm_head'\n",
    "\n",
    "    if isinstance(module, (nn.Linear, LinearCSRForward)):\n",
    "        # Attention\n",
    "        if 'attn' in lname or 'attention' in lname or 'self_attn' in lname:\n",
    "            return 'attention_linear'\n",
    "        # MLP\n",
    "        if 'mlp' in lname or 'ff' in lname or 'fc1' in lname or 'fc2' in lname:\n",
    "            return 'mlp_linear'\n",
    "        return 'other_linear'\n",
    "\n",
    "    if 'norm' in lname:\n",
    "        return 'norm'\n",
    "\n",
    "    return 'other'\n",
    "\n",
    "def analyze_layers(model: nn.Module, label: str):\n",
    "    rows = []\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if name == '':\n",
    "            continue\n",
    "\n",
    "        nonzero = 0\n",
    "        total = 0\n",
    "        flops = 0\n",
    "        shape = None\n",
    "\n",
    "        if isinstance(module, LinearCSRForward):\n",
    "            # Read CSR meta-data\n",
    "            nonzero = module.meta_nnz\n",
    "            total = module.meta_total_params\n",
    "            sparsity = module.meta_sparsity\n",
    "            shape = tuple(module.meta_dense_shape.tolist())\n",
    "            # FLOPs ≈ 2 * nnz (mul + add)\n",
    "            flops = 2 * nonzero\n",
    "        else:\n",
    "            # Standard path (nn.Linear, Embedding, etc.)\n",
    "            for p_name, p in module.named_parameters(recurse=False):\n",
    "                if not isinstance(p, torch.Tensor):\n",
    "                    continue\n",
    "                nnz, tot = tensor_stats(p)\n",
    "                nonzero += nnz\n",
    "                total += tot\n",
    "                if p_name in ('weight', 'weight_orig'):\n",
    "                    flops += linear_flops(p)\n",
    "                    if p.dim() == 2:\n",
    "                        shape = tuple(p.shape)\n",
    "\n",
    "            if total == 0:\n",
    "                continue\n",
    "            sparsity = 1.0 - nonzero / total\n",
    "\n",
    "        group = classify_module(name, module)\n",
    "        rows.append({\n",
    "            'model': label,\n",
    "            'module_name': name,\n",
    "            'group': group,\n",
    "            'nonzero': nonzero,\n",
    "            'total': total,\n",
    "            'sparsity': sparsity,\n",
    "            'shape': str(shape) if shape is not None else '',\n",
    "            'flops_per_token': flops\n",
    "        })\n",
    "\n",
    "    df_layers = pd.DataFrame(rows)\n",
    "    total_params_model = df_layers['total'].sum()\n",
    "    df_layers['param_frac'] = df_layers['total'] / total_params_model\n",
    "\n",
    "    df_groups = (\n",
    "        df_layers\n",
    "        .groupby(['model', 'group'], as_index=False)\n",
    "        .agg({'nonzero': 'sum', 'total': 'sum', 'flops_per_token': 'sum'})\n",
    "    )\n",
    "    df_groups['sparsity'] = 1.0 - df_groups['nonzero'] / df_groups['total']\n",
    "    df_groups['param_frac'] = df_groups['total'] / total_params_model\n",
    "\n",
    "    return df_layers, df_groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35327385",
   "metadata": {},
   "source": [
    "## 3. Dense — baseline structure\n",
    "\n",
    "We first analyse the **unpruned dense model**.\n",
    "\n",
    "For this variant we compute for each layer and group:\n",
    "\n",
    "- number of non-zero parameters,\n",
    "- total parameter count,\n",
    "- sparsity,\n",
    "- parameter fraction within the model,\n",
    "- estimated FLOPs per token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d82a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: facebook/opt-125m\n",
      "Dense global stats: {'nonzero': 125238422, 'total': 125239296, 'sparsity': 6.978640314292406e-06, 'size_mb': 477.75}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>group</th>\n",
       "      <th>nonzero</th>\n",
       "      <th>total</th>\n",
       "      <th>flops_per_token</th>\n",
       "      <th>sparsity</th>\n",
       "      <th>param_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dense</td>\n",
       "      <td>mlp_linear</td>\n",
       "      <td>56669121</td>\n",
       "      <td>56669184</td>\n",
       "      <td>113246208</td>\n",
       "      <td>1.111715e-06</td>\n",
       "      <td>0.345864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dense</td>\n",
       "      <td>embedding</td>\n",
       "      <td>40182508</td>\n",
       "      <td>40183296</td>\n",
       "      <td>80366592</td>\n",
       "      <td>1.961014e-05</td>\n",
       "      <td>0.245247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dense</td>\n",
       "      <td>lm_head</td>\n",
       "      <td>38608881</td>\n",
       "      <td>38608896</td>\n",
       "      <td>77217792</td>\n",
       "      <td>3.885115e-07</td>\n",
       "      <td>0.235638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dense</td>\n",
       "      <td>attention_linear</td>\n",
       "      <td>28348393</td>\n",
       "      <td>28348416</td>\n",
       "      <td>56623104</td>\n",
       "      <td>8.113328e-07</td>\n",
       "      <td>0.173016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dense</td>\n",
       "      <td>norm</td>\n",
       "      <td>38400</td>\n",
       "      <td>38400</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model             group   nonzero     total  flops_per_token      sparsity  \\\n",
       "3  Dense        mlp_linear  56669121  56669184        113246208  1.111715e-06   \n",
       "1  Dense         embedding  40182508  40183296         80366592  1.961014e-05   \n",
       "2  Dense           lm_head  38608881  38608896         77217792  3.885115e-07   \n",
       "0  Dense  attention_linear  28348393  28348416         56623104  8.113328e-07   \n",
       "4  Dense              norm     38400     38400                0  0.000000e+00   \n",
       "\n",
       "   param_frac  \n",
       "3    0.345864  \n",
       "1    0.245247  \n",
       "2    0.235638  \n",
       "0    0.173016  \n",
       "4    0.000234  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ..\\results\\structural_layers_bis\\layers_dense.csv\n",
      "Saved: ..\\results\\structural_layers_bis\\groups_dense.csv\n"
     ]
    }
   ],
   "source": [
    "model_dense, tok, model_name = load_fresh()\n",
    "dense_stats = params_size_and_sparsity(model_dense)\n",
    "print('Dense global stats:', dense_stats)\n",
    "\n",
    "df_layers_dense, df_groups_dense = analyze_layers(model_dense, 'Dense')\n",
    "display(df_groups_dense.sort_values('total', ascending=False))\n",
    "\n",
    "dense_layers_csv = os.path.join(STRUCT_DIR, 'layers_dense.csv')\n",
    "dense_groups_csv = os.path.join(STRUCT_DIR, 'groups_dense.csv')\n",
    "df_layers_dense.to_csv(dense_layers_csv, index=False)\n",
    "df_groups_dense.to_csv(dense_groups_csv, index=False)\n",
    "print('Saved:', dense_layers_csv)\n",
    "print('Saved:', dense_groups_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd3ac32",
   "metadata": {},
   "source": [
    "## 4. Masked30 — global magnitude pruning (30%) on prunable linears\n",
    "\n",
    "We now apply global magnitude pruning (30%) on the prunable linear layers:\n",
    "\n",
    "- We prune only the `nn.Linear` modules returned by `select_prunable_linears`.\n",
    "- The `lm_head` is blacklisted.\n",
    "- Embeddings and LayerNorms are never pruned.\n",
    "\n",
    "We then inspect the effect on:\n",
    "\n",
    "- sparsity per layer / per group,\n",
    "- theoretical FLOPs per token per group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "100574b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: facebook/opt-125m\n",
      "Prunable linear layers: 72\n",
      "Masked global stats: {'nonzero': 99743851, 'total': 125239296, 'sparsity': 0.20357384474598128, 'size_mb': 477.75}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>group</th>\n",
       "      <th>nonzero</th>\n",
       "      <th>total</th>\n",
       "      <th>flops_per_token</th>\n",
       "      <th>sparsity</th>\n",
       "      <th>param_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Masked30</td>\n",
       "      <td>mlp_linear</td>\n",
       "      <td>39318068</td>\n",
       "      <td>56669184</td>\n",
       "      <td>113246208</td>\n",
       "      <td>3.061826e-01</td>\n",
       "      <td>0.345864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Masked30</td>\n",
       "      <td>embedding</td>\n",
       "      <td>40182508</td>\n",
       "      <td>40183296</td>\n",
       "      <td>80366592</td>\n",
       "      <td>1.961014e-05</td>\n",
       "      <td>0.245247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Masked30</td>\n",
       "      <td>lm_head</td>\n",
       "      <td>38608881</td>\n",
       "      <td>38608896</td>\n",
       "      <td>77217792</td>\n",
       "      <td>3.885115e-07</td>\n",
       "      <td>0.235638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Masked30</td>\n",
       "      <td>attention_linear</td>\n",
       "      <td>20204875</td>\n",
       "      <td>28348416</td>\n",
       "      <td>56623104</td>\n",
       "      <td>2.872662e-01</td>\n",
       "      <td>0.173016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Masked30</td>\n",
       "      <td>norm</td>\n",
       "      <td>38400</td>\n",
       "      <td>38400</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model             group   nonzero     total  flops_per_token  \\\n",
       "3  Masked30        mlp_linear  39318068  56669184        113246208   \n",
       "1  Masked30         embedding  40182508  40183296         80366592   \n",
       "2  Masked30           lm_head  38608881  38608896         77217792   \n",
       "0  Masked30  attention_linear  20204875  28348416         56623104   \n",
       "4  Masked30              norm     38400     38400                0   \n",
       "\n",
       "       sparsity  param_frac  \n",
       "3  3.061826e-01    0.345864  \n",
       "1  1.961014e-05    0.245247  \n",
       "2  3.885115e-07    0.235638  \n",
       "0  2.872662e-01    0.173016  \n",
       "4  0.000000e+00    0.000234  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ..\\results\\structural_layers_bis\\layers_masked30.csv\n",
      "Saved: ..\\results\\structural_layers_bis\\groups_masked30.csv\n"
     ]
    }
   ],
   "source": [
    "SP = 0.30\n",
    "\n",
    "model_masked, tok_m, _ = load_fresh()\n",
    "layers_prunable = select_prunable_linears(model_masked, blacklist=(\"lm_head\",))\n",
    "print('Prunable linear layers:', len(layers_prunable))\n",
    "\n",
    "apply_global_magnitude_pruning_cpu_safe(layers_prunable, amount=SP)\n",
    "freeze_pruning_(layers_prunable)\n",
    "\n",
    "masked_stats = params_size_and_sparsity(model_masked)\n",
    "print('Masked global stats:', masked_stats)\n",
    "\n",
    "df_layers_masked, df_groups_masked = analyze_layers(model_masked, f'Masked{int(SP*100)}')\n",
    "display(df_groups_masked.sort_values('total', ascending=False))\n",
    "\n",
    "masked_layers_csv = os.path.join(STRUCT_DIR, 'layers_masked30.csv')\n",
    "masked_groups_csv = os.path.join(STRUCT_DIR, 'groups_masked30.csv')\n",
    "df_layers_masked.to_csv(masked_layers_csv, index=False)\n",
    "df_groups_masked.to_csv(masked_groups_csv, index=False)\n",
    "print('Saved:', masked_layers_csv)\n",
    "print('Saved:', masked_groups_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f8741d",
   "metadata": {},
   "source": [
    "## 5. CSR30 — pruning + CSR conversion (GPU-ready)\n",
    "\n",
    "The pipeline for the CSR variant is:\n",
    "\n",
    "1. Load a fresh model.\n",
    "2. Apply the same global 30% pruning on prunable linear layers.\n",
    "3. Call `freeze_pruning_` to materialise the masks.\n",
    "4. Run `convert_linear_weights_to_csr_`.\n",
    "5. Replace all pruned linear layers by `LinearCSRForward` modules.\n",
    "\n",
    "We then repeat the layer- and group-level analysis.\n",
    "\n",
    "> On CPU this is slower, but on GPU this corresponds to the “CSR, GPU-ready” configuration that we will benchmark on Narval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28f42487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: facebook/opt-125m\n",
      "Prunable linear layers (CSR): 72\n",
      "CSR global stats: {'nonzero': 40303852, 'total': 40304640, 'sparsity': 1.9551098831338543e-05, 'size_mb': 153.75}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>group</th>\n",
       "      <th>nonzero</th>\n",
       "      <th>total</th>\n",
       "      <th>flops_per_token</th>\n",
       "      <th>sparsity</th>\n",
       "      <th>param_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CSR30</td>\n",
       "      <td>mlp_linear</td>\n",
       "      <td>39271988</td>\n",
       "      <td>56623104</td>\n",
       "      <td>78543976</td>\n",
       "      <td>3.064317e-01</td>\n",
       "      <td>0.345758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CSR30</td>\n",
       "      <td>embedding</td>\n",
       "      <td>40182508</td>\n",
       "      <td>40183296</td>\n",
       "      <td>80366592</td>\n",
       "      <td>1.961014e-05</td>\n",
       "      <td>0.245371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CSR30</td>\n",
       "      <td>lm_head</td>\n",
       "      <td>38608881</td>\n",
       "      <td>38608896</td>\n",
       "      <td>77217792</td>\n",
       "      <td>3.885115e-07</td>\n",
       "      <td>0.235758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CSR30</td>\n",
       "      <td>attention_linear</td>\n",
       "      <td>20168011</td>\n",
       "      <td>28311552</td>\n",
       "      <td>40336022</td>\n",
       "      <td>2.876402e-01</td>\n",
       "      <td>0.172879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CSR30</td>\n",
       "      <td>norm</td>\n",
       "      <td>38400</td>\n",
       "      <td>38400</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model             group   nonzero     total  flops_per_token      sparsity  \\\n",
       "3  CSR30        mlp_linear  39271988  56623104         78543976  3.064317e-01   \n",
       "1  CSR30         embedding  40182508  40183296         80366592  1.961014e-05   \n",
       "2  CSR30           lm_head  38608881  38608896         77217792  3.885115e-07   \n",
       "0  CSR30  attention_linear  20168011  28311552         40336022  2.876402e-01   \n",
       "4  CSR30              norm     38400     38400                0  0.000000e+00   \n",
       "\n",
       "   param_frac  \n",
       "3    0.345758  \n",
       "1    0.245371  \n",
       "2    0.235758  \n",
       "0    0.172879  \n",
       "4    0.000234  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ..\\results\\structural_layers_bis\\layers_csr30.csv\n",
      "Saved: ..\\results\\structural_layers_bis\\groups_csr30.csv\n"
     ]
    }
   ],
   "source": [
    "model_csr, tok_c, _ = load_fresh()\n",
    "layers_prunable_csr = select_prunable_linears(model_csr, blacklist=(\"lm_head\",))\n",
    "print('Prunable linear layers (CSR):', len(layers_prunable_csr))\n",
    "\n",
    "apply_global_magnitude_pruning_cpu_safe(layers_prunable_csr, amount=SP)\n",
    "freeze_pruning_(layers_prunable_csr)\n",
    "convert_linear_weights_to_csr_(layers_prunable_csr)\n",
    "\n",
    "# Replace all pruned layers with LinearCSRForward\n",
    "def find_parent(root, child):\n",
    "    for _, mod in root.named_modules():\n",
    "        for cn, cc in mod.named_children():\n",
    "            if cc is child:\n",
    "                return mod, cn\n",
    "    raise RuntimeError('Parent not found')\n",
    "\n",
    "for lin in layers_prunable_csr:\n",
    "    parent, attr = find_parent(model_csr, lin)\n",
    "    csr_module = LinearCSRForward(\n",
    "        lin.weight.detach(),\n",
    "        lin.bias.detach() if lin.bias is not None else None\n",
    "    ).to(device)\n",
    "    setattr(parent, attr, csr_module)\n",
    "\n",
    "csr_stats = params_size_and_sparsity(model_csr)\n",
    "print('CSR global stats:', csr_stats)\n",
    "\n",
    "df_layers_csr, df_groups_csr = analyze_layers(model_csr, f'CSR{int(SP*100)}')\n",
    "display(df_groups_csr.sort_values('total', ascending=False))\n",
    "\n",
    "csr_layers_csv = os.path.join(STRUCT_DIR, 'layers_csr30.csv')\n",
    "csr_groups_csv = os.path.join(STRUCT_DIR, 'groups_csr30.csv')\n",
    "df_layers_csr.to_csv(csr_layers_csv, index=False)\n",
    "df_groups_csr.to_csv(csr_groups_csv, index=False)\n",
    "print('Saved:', csr_layers_csv)\n",
    "print('Saved:', csr_groups_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160b56d7",
   "metadata": {},
   "source": [
    "## 6. Global comparison: Dense vs Masked30 vs CSR30\n",
    "\n",
    "We merge the per-group DataFrames to build an overview with:\n",
    "\n",
    "- sparsity per group and per variant,\n",
    "- parameter share per group and per variant,\n",
    "- total FLOPs per token per group and per variant.\n",
    "\n",
    "This makes it easy to see where parameters and compute are concentrated, and how pruning + CSR change the picture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e867b7a3-0e38-4b31-862f-ef80d3957713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grouped(pivot_df, title, ylabel, filename, fmt='{:.3f}'):\n",
    "    groups = list(pivot_df.index)\n",
    "    variants = list(pivot_df.columns)\n",
    "    x = list(range(len(groups)))\n",
    "    width = 0.8 / max(1, len(variants))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    for i, var in enumerate(variants):\n",
    "        values = pivot_df[var].values\n",
    "        offset = (i - (len(variants) - 1) / 2) * width\n",
    "        xs = [xi + offset for xi in x]\n",
    "        bars = ax.bar(xs, values, width, label=var)\n",
    "        # annotations\n",
    "        for bx, bv in zip(xs, values):\n",
    "            ax.text(bx, bv, fmt.format(bv), ha='center', va='bottom', fontsize=8, rotation=0)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(groups, rotation=30, ha='right')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    path = os.path.join(STRUCT_DIR, filename)\n",
    "    fig.savefig(path, dpi=150)\n",
    "    plt.close(fig)\n",
    "    print('Saved plot:', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da26530e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>group</th>\n",
       "      <th>nonzero</th>\n",
       "      <th>total</th>\n",
       "      <th>flops_per_token</th>\n",
       "      <th>sparsity</th>\n",
       "      <th>param_frac</th>\n",
       "      <th>variant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CSR30</td>\n",
       "      <td>attention_linear</td>\n",
       "      <td>20168011</td>\n",
       "      <td>28311552</td>\n",
       "      <td>40336022</td>\n",
       "      <td>2.876402e-01</td>\n",
       "      <td>0.172879</td>\n",
       "      <td>CSR30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dense</td>\n",
       "      <td>attention_linear</td>\n",
       "      <td>28348393</td>\n",
       "      <td>28348416</td>\n",
       "      <td>56623104</td>\n",
       "      <td>8.113328e-07</td>\n",
       "      <td>0.173016</td>\n",
       "      <td>Dense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Masked30</td>\n",
       "      <td>attention_linear</td>\n",
       "      <td>20204875</td>\n",
       "      <td>28348416</td>\n",
       "      <td>56623104</td>\n",
       "      <td>2.872662e-01</td>\n",
       "      <td>0.173016</td>\n",
       "      <td>Masked30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CSR30</td>\n",
       "      <td>embedding</td>\n",
       "      <td>40182508</td>\n",
       "      <td>40183296</td>\n",
       "      <td>80366592</td>\n",
       "      <td>1.961014e-05</td>\n",
       "      <td>0.245371</td>\n",
       "      <td>CSR30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dense</td>\n",
       "      <td>embedding</td>\n",
       "      <td>40182508</td>\n",
       "      <td>40183296</td>\n",
       "      <td>80366592</td>\n",
       "      <td>1.961014e-05</td>\n",
       "      <td>0.245247</td>\n",
       "      <td>Dense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Masked30</td>\n",
       "      <td>embedding</td>\n",
       "      <td>40182508</td>\n",
       "      <td>40183296</td>\n",
       "      <td>80366592</td>\n",
       "      <td>1.961014e-05</td>\n",
       "      <td>0.245247</td>\n",
       "      <td>Masked30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CSR30</td>\n",
       "      <td>lm_head</td>\n",
       "      <td>38608881</td>\n",
       "      <td>38608896</td>\n",
       "      <td>77217792</td>\n",
       "      <td>3.885115e-07</td>\n",
       "      <td>0.235758</td>\n",
       "      <td>CSR30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dense</td>\n",
       "      <td>lm_head</td>\n",
       "      <td>38608881</td>\n",
       "      <td>38608896</td>\n",
       "      <td>77217792</td>\n",
       "      <td>3.885115e-07</td>\n",
       "      <td>0.235638</td>\n",
       "      <td>Dense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Masked30</td>\n",
       "      <td>lm_head</td>\n",
       "      <td>38608881</td>\n",
       "      <td>38608896</td>\n",
       "      <td>77217792</td>\n",
       "      <td>3.885115e-07</td>\n",
       "      <td>0.235638</td>\n",
       "      <td>Masked30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CSR30</td>\n",
       "      <td>mlp_linear</td>\n",
       "      <td>39271988</td>\n",
       "      <td>56623104</td>\n",
       "      <td>78543976</td>\n",
       "      <td>3.064317e-01</td>\n",
       "      <td>0.345758</td>\n",
       "      <td>CSR30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dense</td>\n",
       "      <td>mlp_linear</td>\n",
       "      <td>56669121</td>\n",
       "      <td>56669184</td>\n",
       "      <td>113246208</td>\n",
       "      <td>1.111715e-06</td>\n",
       "      <td>0.345864</td>\n",
       "      <td>Dense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Masked30</td>\n",
       "      <td>mlp_linear</td>\n",
       "      <td>39318068</td>\n",
       "      <td>56669184</td>\n",
       "      <td>113246208</td>\n",
       "      <td>3.061826e-01</td>\n",
       "      <td>0.345864</td>\n",
       "      <td>Masked30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CSR30</td>\n",
       "      <td>norm</td>\n",
       "      <td>38400</td>\n",
       "      <td>38400</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>CSR30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dense</td>\n",
       "      <td>norm</td>\n",
       "      <td>38400</td>\n",
       "      <td>38400</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>Dense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Masked30</td>\n",
       "      <td>norm</td>\n",
       "      <td>38400</td>\n",
       "      <td>38400</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>Masked30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model             group   nonzero     total  flops_per_token  \\\n",
       "10     CSR30  attention_linear  20168011  28311552         40336022   \n",
       "0      Dense  attention_linear  28348393  28348416         56623104   \n",
       "5   Masked30  attention_linear  20204875  28348416         56623104   \n",
       "11     CSR30         embedding  40182508  40183296         80366592   \n",
       "1      Dense         embedding  40182508  40183296         80366592   \n",
       "6   Masked30         embedding  40182508  40183296         80366592   \n",
       "12     CSR30           lm_head  38608881  38608896         77217792   \n",
       "2      Dense           lm_head  38608881  38608896         77217792   \n",
       "7   Masked30           lm_head  38608881  38608896         77217792   \n",
       "13     CSR30        mlp_linear  39271988  56623104         78543976   \n",
       "3      Dense        mlp_linear  56669121  56669184        113246208   \n",
       "8   Masked30        mlp_linear  39318068  56669184        113246208   \n",
       "14     CSR30              norm     38400     38400                0   \n",
       "4      Dense              norm     38400     38400                0   \n",
       "9   Masked30              norm     38400     38400                0   \n",
       "\n",
       "        sparsity  param_frac   variant  \n",
       "10  2.876402e-01    0.172879     CSR30  \n",
       "0   8.113328e-07    0.173016     Dense  \n",
       "5   2.872662e-01    0.173016  Masked30  \n",
       "11  1.961014e-05    0.245371     CSR30  \n",
       "1   1.961014e-05    0.245247     Dense  \n",
       "6   1.961014e-05    0.245247  Masked30  \n",
       "12  3.885115e-07    0.235758     CSR30  \n",
       "2   3.885115e-07    0.235638     Dense  \n",
       "7   3.885115e-07    0.235638  Masked30  \n",
       "13  3.064317e-01    0.345758     CSR30  \n",
       "3   1.111715e-06    0.345864     Dense  \n",
       "8   3.061826e-01    0.345864  Masked30  \n",
       "14  0.000000e+00    0.000234     CSR30  \n",
       "4   0.000000e+00    0.000234     Dense  \n",
       "9   0.000000e+00    0.000234  Masked30  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparsity per group:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variant</th>\n",
       "      <th>CSR30</th>\n",
       "      <th>Dense</th>\n",
       "      <th>Masked30</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>attention_linear</th>\n",
       "      <td>2.876402e-01</td>\n",
       "      <td>8.113328e-07</td>\n",
       "      <td>2.872662e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding</th>\n",
       "      <td>1.961014e-05</td>\n",
       "      <td>1.961014e-05</td>\n",
       "      <td>1.961014e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lm_head</th>\n",
       "      <td>3.885115e-07</td>\n",
       "      <td>3.885115e-07</td>\n",
       "      <td>3.885115e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_linear</th>\n",
       "      <td>3.064317e-01</td>\n",
       "      <td>1.111715e-06</td>\n",
       "      <td>3.061826e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variant                  CSR30         Dense      Masked30\n",
       "group                                                     \n",
       "attention_linear  2.876402e-01  8.113328e-07  2.872662e-01\n",
       "embedding         1.961014e-05  1.961014e-05  1.961014e-05\n",
       "lm_head           3.885115e-07  3.885115e-07  3.885115e-07\n",
       "mlp_linear        3.064317e-01  1.111715e-06  3.061826e-01\n",
       "norm              0.000000e+00  0.000000e+00  0.000000e+00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameter share per group:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variant</th>\n",
       "      <th>CSR30</th>\n",
       "      <th>Dense</th>\n",
       "      <th>Masked30</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>attention_linear</th>\n",
       "      <td>0.172879</td>\n",
       "      <td>0.173016</td>\n",
       "      <td>0.173016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding</th>\n",
       "      <td>0.245371</td>\n",
       "      <td>0.245247</td>\n",
       "      <td>0.245247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lm_head</th>\n",
       "      <td>0.235758</td>\n",
       "      <td>0.235638</td>\n",
       "      <td>0.235638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_linear</th>\n",
       "      <td>0.345758</td>\n",
       "      <td>0.345864</td>\n",
       "      <td>0.345864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm</th>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variant              CSR30     Dense  Masked30\n",
       "group                                         \n",
       "attention_linear  0.172879  0.173016  0.173016\n",
       "embedding         0.245371  0.245247  0.245247\n",
       "lm_head           0.235758  0.235638  0.235638\n",
       "mlp_linear        0.345758  0.345864  0.345864\n",
       "norm              0.000234  0.000234  0.000234"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FLOPs per token per group:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variant</th>\n",
       "      <th>CSR30</th>\n",
       "      <th>Dense</th>\n",
       "      <th>Masked30</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>attention_linear</th>\n",
       "      <td>40336022</td>\n",
       "      <td>56623104</td>\n",
       "      <td>56623104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding</th>\n",
       "      <td>80366592</td>\n",
       "      <td>80366592</td>\n",
       "      <td>80366592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lm_head</th>\n",
       "      <td>77217792</td>\n",
       "      <td>77217792</td>\n",
       "      <td>77217792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_linear</th>\n",
       "      <td>78543976</td>\n",
       "      <td>113246208</td>\n",
       "      <td>113246208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variant              CSR30      Dense   Masked30\n",
       "group                                           \n",
       "attention_linear  40336022   56623104   56623104\n",
       "embedding         80366592   80366592   80366592\n",
       "lm_head           77217792   77217792   77217792\n",
       "mlp_linear        78543976  113246208  113246208\n",
       "norm                     0          0          0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot: ..\\results\\structural_layers_bis\\all_sparsity_per_group.png\n",
      "Saved plot: ..\\results\\structural_layers_bis\\all_param_frac_per_group.png\n",
      "Saved plot: ..\\results\\structural_layers_bis\\all_flops_per_group.png\n"
     ]
    }
   ],
   "source": [
    "dfg_d = df_groups_dense.copy();   dfg_d['variant'] = 'Dense'\n",
    "dfg_m = df_groups_masked.copy();  dfg_m['variant'] = 'Masked30'\n",
    "dfg_c = df_groups_csr.copy();     dfg_c['variant'] = 'CSR30'\n",
    "\n",
    "dfg_all = pd.concat([dfg_d, dfg_m, dfg_c], ignore_index=True)\n",
    "# dfg_all = pd.concat([dfg_d, dfg_m], ignore_index=True)\n",
    "display(dfg_all.sort_values(['group', 'variant']))\n",
    "\n",
    "pivot_sparsity = dfg_all.pivot(index='group', columns='variant', values='sparsity').fillna(0.0)\n",
    "pivot_frac = dfg_all.pivot(index='group', columns='variant', values='param_frac').fillna(0.0)\n",
    "pivot_flops = dfg_all.pivot(index='group', columns='variant', values='flops_per_token').fillna(0.0)\n",
    "\n",
    "print('\\nSparsity per group:')\n",
    "display(pivot_sparsity)\n",
    "print('\\nParameter share per group:')\n",
    "display(pivot_frac)\n",
    "print('\\nFLOPs per token per group:')\n",
    "display(pivot_flops)\n",
    "\n",
    "# 3 plots globaux avec infos riches\n",
    "plot_grouped(pivot_sparsity, 'Sparsity per group (Dense vs Masked30 vs CSR30)', 'sparsity', 'all_sparsity_per_group.png', fmt='{:.3f}')\n",
    "plot_grouped(pivot_frac, 'Parameter share per group', 'parameter fraction', 'all_param_frac_per_group.png', fmt='{:.3f}')\n",
    "plot_grouped(pivot_flops, 'FLOPs per token per group', 'FLOPs per token', 'all_flops_per_group.png', fmt='{:.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf936a28",
   "metadata": {},
   "source": [
    "## 7. Using these results in the report\n",
    "\n",
    "With this notebook you can document, with quantitative evidence:\n",
    "\n",
    "- **Parameter distribution** across groups: embeddings vs attention vs MLP vs `lm_head` (`param_frac`).\n",
    "- **Where sparsity actually appears**: `sparsity` per group, highlighting that embeddings and the output head typically remain dense.\n",
    "- **Impact on theoretical compute**: `flops_per_token` per group and per variant, which lets you argue about potential FLOPs savings if CSR kernels are efficient.\n",
    "\n",
    "You can also zoom in *per layer* using the `df_layers_*` tables to show, for example, that:\n",
    "\n",
    "- MLP layers often contain more parameters and benefit more from pruning than attention projections.\n",
    "- Some early or late layers are more sensitive to pruning (which you can correlate with top-1 accuracy or perplexity experiments).\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
