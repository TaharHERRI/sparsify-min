{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad99b75e",
   "metadata": {
    "id": "ad99b75e"
   },
   "source": [
    "# Minimal: Dense → Masked → CSR (similarity and structure)\n",
    "\n",
    "This notebook provides a small, readable baseline comparison between three variants of the same model:\n",
    "\n",
    "- **Dense**\n",
    "- **Masked 30%** (unstructured magnitude pruning, still executed with dense kernels)\n",
    "- **CSR 30%** (same pruning, but converted to CSR and executed with sparse kernels)\n",
    "\n",
    "We compare:\n",
    "\n",
    "- **Structure:** model size (MB), global sparsity, relative compute (nnz (nb of non-zero weights) / dense)\n",
    "- **Behaviour:** *top-1 agreement* with the dense model (percentage of tokens for which the predicted class is identical).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b356c",
   "metadata": {},
   "source": [
    "\n",
    "## What do the metrics mean in this notebook?\n",
    "\n",
    "This notebook compares three variants of the same model:\n",
    "\n",
    "- **Dense** – original model, no pruning.\n",
    "- **Masked 30%** – global magnitude pruning: ~30% of weights in prunable linear layers are set to zero, but tensors stay dense.\n",
    "- **CSR 30%** – same pruning level, but selected linear layers are converted to a **CSR (Compressed Sparse Row)** representation and executed with a sparse kernel (`LinearCSRForward`).\n",
    "\n",
    "For each variant we log four key metrics:\n",
    "\n",
    "### 1. `size_mb`\n",
    "\n",
    "Approximate **model size in megabytes**:\n",
    "\n",
    "- Computed from the number of parameters and their data type (fp32, fp16, …).\n",
    "- Tells you how much **memory** the model occupies on disk / in RAM / on the GPU.\n",
    "- In this notebook:\n",
    "  - **Dense** and **Masked30** have almost the same `size_mb`, because masking does **not** physically remove zeros.\n",
    "  - **CSR30** is smaller, because only the non-zero values and sparse indices are stored for converted layers.\n",
    "\n",
    "### 2. `sparsity`\n",
    "\n",
    "Global **fraction of zero weights** in the entire model:\n",
    "\n",
    "\\[\n",
    "\\text{sparsity} = 1 - \\frac{\\text{nonzero}}{\\text{total}}\n",
    "\\]\n",
    "\n",
    "- `nonzero`: number of parameters that are not equal to zero.\n",
    "- `total`: total number of parameters.\n",
    "- A higher `sparsity` means more zeros.\n",
    "- Here you can see that:\n",
    "  - **Dense** has almost zero sparsity (baseline).\n",
    "  - **Masked30** has ~20% sparsity at the model level (only some layers are pruned).\n",
    "  - **CSR30** appears almost dense again, because only CSR layers are stored sparsely; embeddings and other dense parts remain.\n",
    "\n",
    "### 3. `compute_ratio`\n",
    "\n",
    "Approximate **relative compute cost** assuming that each non-zero weight contributes one unit of work:\n",
    "\n",
    "\\[\n",
    "\\text{compute_ratio} = \\frac{\\text{nonzero (variant)}}{\\text{nonzero (Dense)}}\n",
    "\\]\n",
    "\n",
    "- This is a very simple *theoretical* proxy:\n",
    "  - If `compute_ratio = 0.8`, we expect ~20% fewer FLOPs than the dense baseline (if kernels were perfectly sparse-aware).\n",
    "- In practice:\n",
    "  - **Masked30**: fewer non-zero weights, but tensors are still dense → most BLAS kernels will still perform *dense* computation.\n",
    "  - **CSR30**: non-zero weights are stored in CSR and used in `LinearCSRForward`, so compute cost is much closer to the number of non-zeros.\n",
    "\n",
    "### 4. `top1_match`\n",
    "\n",
    "**Top‑1 agreement** between a variant and the dense reference on the same evaluation texts:\n",
    "\n",
    "1. We take `SAMPLE_TEXTS` (a small, fixed corpus of sentences).\n",
    "2. For each model, we compute logits over all positions.\n",
    "3. For each token position, we compare the *argmax* (most probable token) of:\n",
    "   - the dense baseline `logits_ref`, and\n",
    "   - the tested variant `logits_test`.\n",
    "4. We only count positions where the attention mask is 1 (i.e., real tokens, not padding).\n",
    "5. `top1_match` is the fraction of positions where the two models predict the **same token**:\n",
    "\n",
    "$\\text{top1\\_match} = \\mathbb{E}[\\mathbf{1}\\{ \\arg\\max \\text{Dense} = \\arg\\max \\text{Variant} \\}]$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- `top1_match = 1.0` for **Dense** (same model vs itself).\n",
    "- A high `top1_match` for **Masked30** / **CSR30** means that pruning and CSR conversion preserve the behaviour of the dense model on this small corpus.\n",
    "- A drop in `top1_match` quantifies how much pruning changes the predictions, which complements other metrics like perplexity.\n",
    "\n",
    "This notebook is therefore mainly about:\n",
    "\n",
    "- **Structure and memory**: `size_mb`, `sparsity`, `compute_ratio`.\n",
    "- **Behavioural similarity**: `top1_match` vs Dense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ILE-ENu_uJv_",
   "metadata": {
    "id": "ILE-ENu_uJv_"
   },
   "outputs": [],
   "source": [
    "# For Google Colab (comment out if you are running locally)\n",
    "# %cd /content/Edu-Sparsify-LLMs/notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4785b8cd",
   "metadata": {
    "id": "4785b8cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, sys, warnings, pandas as pd, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "sys.path.append('..'); sys.path.append('../src')\n",
    "\n",
    "from src.eval.metrics import params_size_and_sparsity\n",
    "from src.eval.csvlog import append_row\n",
    "from src.eval.plotting import bar_plot\n",
    "from src.pruning.policies import apply_global_magnitude_pruning_cpu_safe, select_prunable_linears\n",
    "from src.pruning.pipeline import freeze_pruning_, convert_linear_weights_to_csr_\n",
    "from src.wrappers.linear_csr import LinearCSRForward\n",
    "\n",
    "warnings.filterwarnings('ignore', message='.*Sparse CSR tensor support is in beta state.*')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "RESULTS_DIR = os.path.join('..', 'results')\n",
    "CSV_PATH = os.path.join(RESULTS_DIR, 'S1_minimal_similarity.csv')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# We log: setup, size, sparsity, compute ratio, top-1 similarity\n",
    "pd.DataFrame(columns=[\n",
    "    \"setup\",\n",
    "    \"size_mb\",\n",
    "    \"sparsity\",\n",
    "    \"compute_ratio\",\n",
    "    \"top1_match\"\n",
    "]).to_csv(CSV_PATH, index=False)\n",
    "\n",
    "def load_fresh():\n",
    "    \"\"\"Load a small model depending on the device.\n",
    "\n",
    "    - CUDA -> EleutherAI/pythia-410m (fp16)\n",
    "    - CPU  -> facebook/opt-125m     (fp32)\n",
    "    \"\"\"\n",
    "    if device == \"cuda\":\n",
    "        model_name = \"EleutherAI/pythia-410m\"\n",
    "        torch_dtype = torch.float16\n",
    "    else:\n",
    "        model_name = \"facebook/opt-125m\"\n",
    "        torch_dtype = None  # default fp32\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "    kwargs = {}\n",
    "    if torch_dtype is not None:\n",
    "        kwargs[\"torch_dtype\"] = torch_dtype\n",
    "\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(model_name, **kwargs).to(device).eval()\n",
    "    print(f\"Loaded: {model_name}\")\n",
    "    return mdl, tok, model_name\n",
    "\n",
    "def collect_logits(model, tokenizer, texts, device):\n",
    "    \"\"\"Return (logits, attention_mask) for a list of texts.\n",
    "\n",
    "    logits: [B, L, V], mask: [B, L]\n",
    "    \"\"\"\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    with torch.inference_mode():\n",
    "        out = model(**enc)\n",
    "    logits = out.logits.cpu()\n",
    "    mask = enc[\"attention_mask\"].cpu()\n",
    "    return logits, mask\n",
    "\n",
    "def top1_agreement(logits_ref, logits_test, att_mask):\n",
    "    \"\"\"Percentage of tokens with identical top-1 predictions between\n",
    "    a reference model and a tested model.\n",
    "    \"\"\"\n",
    "    ref = logits_ref.argmax(dim=-1)   # [B, L]\n",
    "    test = logits_test.argmax(dim=-1) # [B, L]\n",
    "    mask = att_mask.bool()\n",
    "    match = (ref[mask] == test[mask]).float().mean().item()\n",
    "    return match\n",
    "\n",
    "SAMPLE_TEXTS = [\n",
    "    \"In a quiet valley, the river bends slowly around the last farm before the hills.\",\n",
    "    \"Sparse pruning zeroes weights but needs a sparse kernel to speed up compute.\",\n",
    "    \"A small batch size can distort latency because of cache and warmup effects.\",\n",
    "    \"Causal LM perplexity is averaged per token over sliding blocks.\",\n",
    "    \"Version 1.2.0 fixes: stability on CPU, deterministic seeds, better logging.\",\n",
    "    \"\\\"Hello?\\\" — \\\"Hi; can you hear me?\\\" — \\\"Loud and clear.\\\"\",\n",
    "    \"HTTP 429 means rate limiting; use exponential backoff with jitter.\",\n",
    "    \"Compute follows memory: fewer bytes moved often means fewer milliseconds.\",\n",
    "    \"Numbers: 3.14159, 2.71828, 0.57721 show up in odd places.\",\n",
    "    \"Keep the same corpus when comparing Dense vs Masked vs CSR.\",\n",
    "    \"If latency jumps, check power limits, thermal throttling, governors.\",\n",
    "    \"We log mean, median, and p95 latency because tails matter.\",\n",
    "    \"One batch isn’t enough: run multiple iterations with warmup.\",\n",
    "    \"Tiny masking mistakes can create NaNs; clamp logits if needed.\",\n",
    "    \"When in doubt, profile with both synthetic and real inputs.\"\n",
    "]\n",
    "# We can virtually increase the corpus size if needed\n",
    "# SAMPLE_TEXTS = SAMPLE_TEXTS * 20\n",
    "\n",
    "# Global variables for the dense reference\n",
    "DENSE_NONZERO = None\n",
    "DENSE_LOGITS = None\n",
    "DENSE_MASK = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3ecb3",
   "metadata": {
    "id": "b9c3ecb3"
   },
   "source": [
    "## 1) Dense baseline\n",
    "\n",
    "In this first step we:\n",
    "\n",
    "- measure the model size and (near-zero) global sparsity;\n",
    "- store:\n",
    "  - the number of non-zero parameters (`DENSE_NONZERO`),\n",
    "  - the reference logits on `SAMPLE_TEXTS` (`DENSE_LOGITS`, `DENSE_MASK`).\n",
    "\n",
    "By definition for the dense baseline:\n",
    "\n",
    "- `compute_ratio = 1.0` (reference compute), and  \n",
    "- `top1_match = 1.0` (dense vs. dense).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bf0833b",
   "metadata": {
    "id": "5bf0833b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: facebook/opt-125m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "C:\\Users\\masra\\OneDrive\\Images\\Edu-Sparsify-LLMs\\notebooks\\..\\src\\eval\\csvlog.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'nonzero': 125238422,\n",
       "  'total': 125239296,\n",
       "  'sparsity': 6.978640314292406e-06,\n",
       "  'size_mb': 477.75},\n",
       " 1.0,\n",
       " 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global DENSE_NONZERO, DENSE_LOGITS, DENSE_MASK\n",
    "\n",
    "model, tok, name = load_fresh()\n",
    "stats = params_size_and_sparsity(model)\n",
    "\n",
    "# Reference logits on the small corpus\n",
    "DENSE_LOGITS, DENSE_MASK = collect_logits(model, tok, SAMPLE_TEXTS, device)\n",
    "DENSE_NONZERO = stats['nonzero']\n",
    "\n",
    "compute_ratio = 1.0\n",
    "top1 = 1.0\n",
    "\n",
    "append_row(\n",
    "    CSV_PATH,\n",
    "    setup='Dense',\n",
    "    size_mb=stats['size_mb'],\n",
    "    sparsity=stats['sparsity'],\n",
    "    compute_ratio=compute_ratio,\n",
    "    top1_match=top1\n",
    ")\n",
    "\n",
    "stats, compute_ratio, top1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0098f2",
   "metadata": {
    "id": "de0098f2"
   },
   "source": [
    "## 2) Masked pruning (30%) — dense execution\n",
    "\n",
    "Here we apply **global magnitude pruning at 30%** on the selected linear layers.\n",
    "\n",
    "- The linear layers remain in **dense** format (weights are masked, but kernels are dense).\n",
    "- We measure:\n",
    "  - global sparsity,\n",
    "  - effective model size (should be ~identical to Dense),\n",
    "  - `compute_ratio = nnz_sparse / DENSE_NONZERO`,\n",
    "  - `top1_match` vs. the dense model on `SAMPLE_TEXTS`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5365922b",
   "metadata": {
    "id": "5365922b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: facebook/opt-125m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'nonzero': 99743851,\n",
       "  'total': 125239296,\n",
       "  'sparsity': 0.20357384474598128,\n",
       "  'size_mb': 477.75},\n",
       " 0.7964317132644805,\n",
       " 0.6613546013832092)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert DENSE_NONZERO is not None, \"Run the Dense baseline cell first.\"\n",
    "\n",
    "SP_MASK = 0.30\n",
    "model, tok, name = load_fresh()\n",
    "\n",
    "layers = select_prunable_linears(model, blacklist=(\"lm_head\",))\n",
    "apply_global_magnitude_pruning_cpu_safe(layers, amount=SP_MASK)\n",
    "freeze_pruning_(layers)\n",
    "\n",
    "stats = params_size_and_sparsity(model)\n",
    "logits_masked, _ = collect_logits(model, tok, SAMPLE_TEXTS, device)\n",
    "\n",
    "compute_ratio = stats['nonzero'] / DENSE_NONZERO\n",
    "top1 = top1_agreement(DENSE_LOGITS, logits_masked, DENSE_MASK)\n",
    "\n",
    "append_row(\n",
    "    CSV_PATH,\n",
    "    setup=f'Masked{int(SP_MASK*100)}',\n",
    "    size_mb=stats['size_mb'],\n",
    "    sparsity=stats['sparsity'],\n",
    "    compute_ratio=compute_ratio,\n",
    "    top1_match=top1\n",
    ")\n",
    "\n",
    "stats, compute_ratio, top1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc76a8de",
   "metadata": {
    "id": "dc76a8de"
   },
   "source": [
    "## 3) CSR execution (30%) — truly sparse kernels\n",
    "\n",
    "We repeat the same global 30% pruning, then:\n",
    "\n",
    "1. freeze the pruning masks,\n",
    "2. convert the pruned linear layers to CSR,\n",
    "3. replace the corresponding modules with `LinearCSRForward`.\n",
    "\n",
    "We again measure:\n",
    "\n",
    "- model size and global sparsity,\n",
    "- `compute_ratio` (now reflecting the CSR parameter count),\n",
    "- `top1_match` vs. the dense model on `SAMPLE_TEXTS`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41b1330a",
   "metadata": {
    "id": "41b1330a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: facebook/opt-125m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "C:\\Users\\masra\\OneDrive\\Images\\Edu-Sparsify-LLMs\\notebooks\\..\\src\\wrappers\\linear_csr.py:16: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4419.)\n",
      "  out = torch.matmul(W, x.T).T               # [out,b] -> [b,out]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'nonzero': 40220908,\n",
       "  'total': 40221696,\n",
       "  'sparsity': 1.959141653296026e-05,\n",
       "  'size_mb': 153.43359375},\n",
       " 0.3211547012305856,\n",
       " 0.6613546013832092)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert DENSE_NONZERO is not None, \"Run the Dense baseline cell first.\"\n",
    "\n",
    "SP_CSR = 0.30\n",
    "model, tok, name = load_fresh()\n",
    "\n",
    "layers = select_prunable_linears(model, blacklist=(\"lm_head\",))\n",
    "apply_global_magnitude_pruning_cpu_safe(layers, amount=SP_CSR)\n",
    "freeze_pruning_(layers)\n",
    "convert_linear_weights_to_csr_(layers)\n",
    "\n",
    "# Replace all pruned linear layers by LinearCSRForward\n",
    "swapped = 0\n",
    "def find_parent(root, child):\n",
    "    for _, mod in root.named_modules():\n",
    "        for cn, cc in mod.named_children():\n",
    "            if cc is child:\n",
    "                return mod, cn\n",
    "    raise RuntimeError('Parent not found')\n",
    "\n",
    "for lin in layers:\n",
    "    # Uncomment to restrict to a subset of layers for quick demos\n",
    "    # if swapped >= 4:\n",
    "    #     break\n",
    "    parent, attr = find_parent(model, lin)\n",
    "    csr_module = LinearCSRForward(\n",
    "        lin.weight.detach(),\n",
    "        lin.bias.detach() if lin.bias is not None else None\n",
    "    ).to(device)\n",
    "    setattr(parent, attr, csr_module)\n",
    "    swapped += 1\n",
    "\n",
    "stats = params_size_and_sparsity(model)\n",
    "logits_csr, _ = collect_logits(model, tok, SAMPLE_TEXTS, device)\n",
    "\n",
    "compute_ratio = stats['nonzero'] / DENSE_NONZERO\n",
    "top1 = top1_agreement(DENSE_LOGITS, logits_csr, DENSE_MASK)\n",
    "\n",
    "append_row(\n",
    "    CSV_PATH,\n",
    "    setup=f'CSR{int(SP_CSR*100)}',\n",
    "    size_mb=stats['size_mb'],\n",
    "    sparsity=stats['sparsity'],\n",
    "    compute_ratio=compute_ratio,\n",
    "    top1_match=top1\n",
    ")\n",
    "\n",
    "stats, compute_ratio, top1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba329b41",
   "metadata": {
    "id": "ba329b41"
   },
   "source": [
    "## 4) Plots\n",
    "\n",
    "We visualise the three variants (**Dense**, **Masked30**, **CSR30**) with simple bar plots:\n",
    "\n",
    "- **Model size (MB)** per setup.\n",
    "- **Relative compute** (`nnz / dense`) per setup.\n",
    "- **Top-1 agreement** with the dense model per setup.\n",
    "\n",
    "These plots give a compact view of the trade-off between sparsity, model size, effective compute and predictive similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "196a989c",
   "metadata": {
    "id": "196a989c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setup</th>\n",
       "      <th>size_mb</th>\n",
       "      <th>sparsity</th>\n",
       "      <th>compute_ratio</th>\n",
       "      <th>top1_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dense</td>\n",
       "      <td>477.750000</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Masked30</td>\n",
       "      <td>477.750000</td>\n",
       "      <td>0.203574</td>\n",
       "      <td>0.796432</td>\n",
       "      <td>0.661355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CSR30</td>\n",
       "      <td>153.433594</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.321155</td>\n",
       "      <td>0.661355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      setup     size_mb  sparsity  compute_ratio  top1_match\n",
       "0     Dense  477.750000  0.000007       1.000000    1.000000\n",
       "1  Masked30  477.750000  0.203574       0.796432    0.661355\n",
       "2     CSR30  153.433594  0.000020       0.321155    0.661355"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ..\\results\\size_vs_sparsity.png\n",
      "Saved: ..\\results\\compute_vs_sparsity.png\n",
      "Saved: ..\\results\\top1_vs_sparsity.png\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "display(df)\n",
    "\n",
    "bar_plot(df, 'setup', 'size_mb', 'Model size (MB)', 'size_vs_sparsity.png', RESULTS_DIR, y_min=None)\n",
    "bar_plot(df, 'setup', 'compute_ratio', 'Relative compute (nnz / dense)', 'compute_vs_sparsity.png', RESULTS_DIR)\n",
    "bar_plot(df, 'setup', 'top1_match', 'Top-1 agreement vs Dense', 'top1_vs_sparsity.png', RESULTS_DIR, y_min=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f4e77-81ea-44ce-8fe6-9fa04f06fd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
